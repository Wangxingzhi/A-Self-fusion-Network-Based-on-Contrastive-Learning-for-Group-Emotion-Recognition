{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os,torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import argparse\n",
    "import torchfile\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "from vggmodel.vggface import VGGFace\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotiWDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_filelist, face_filelist, maxFaces, transformFaces = transforms):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filelist: List of names of image/feature files.\n",
    "            root_dir: Dataset directory\n",
    "            transform (callable, optional): Optional transformer to be applied\n",
    "                                            on an image sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image_filelist = image_filelist\n",
    "        self.face_filelist = face_filelist\n",
    "        self.transformFaces = transformFaces\n",
    "        \n",
    "        neg_filelist = sorted(os.listdir(image_filelist + 'Negative/'))\n",
    "        neu_filelist = sorted(os.listdir(image_filelist + 'Neutral/'))\n",
    "        pos_filelist = sorted(os.listdir(image_filelist + 'Positive/'))\n",
    "        \n",
    "        all_filelist = neg_filelist + neu_filelist + pos_filelist\n",
    "        \n",
    "        self.name_filelist = [x.split('.')[0] for x in all_filelist]\n",
    "\n",
    "        self.label = []\n",
    "        neg_label = np.array(np.zeros(len(neg_filelist)),dtype = np.int64)\n",
    "        neu_label = np.array(np.ones(len(neu_filelist)),dtype = np.int64)\n",
    "        pos_label = np.array(2*np.ones(len(pos_filelist)),dtype = np.int64)\n",
    "        \n",
    "        self.label.extend(neg_label)\n",
    "        self.label.extend(neu_label)\n",
    "        self.label.extend(pos_label)\n",
    "        \n",
    "        self.file_paths = []\n",
    "        \n",
    "        for f in neg_filelist:\n",
    "            path = os.path.join(self.image_filelist,'Negative/',f)\n",
    "            self.file_paths.append(path)\n",
    "        for f in neu_filelist:\n",
    "            path = os.path.join(self.image_filelist,'Neutral/',f)\n",
    "            self.file_paths.append(path)\n",
    "        for f in pos_filelist:\n",
    "            path = os.path.join(self.image_filelist,'Positive/',f)\n",
    "            self.file_paths.append(path)       \n",
    "\n",
    "        neg_face_path = []\n",
    "        neu_face_path = []\n",
    "        pos_face_path = []\n",
    "        \n",
    "        self.all_face_path = []\n",
    "        \n",
    "        neg_path_filelist = [x.split('.')[0] for x in neg_filelist]\n",
    "        neu_path_filelist = [x.split('.')[0] for x in neu_filelist]\n",
    "        pos_path_filelist = [x.split('.')[0] for x in pos_filelist]\n",
    "\n",
    "        for f in neg_path_filelist:      \n",
    "            path = os.path.join(face_filelist,'Negative/',f)\n",
    "            neg_face_path.append(path)    \n",
    "        for f in neu_path_filelist:      \n",
    "            path = os.path.join(face_filelist,'Neutral/',f)\n",
    "            neu_face_path.append(path)    \n",
    "        for f in pos_path_filelist:      \n",
    "            path = os.path.join(face_filelist,'Positive/',f)\n",
    "            pos_face_path.append(path)                \n",
    "        \n",
    "        self.all_face_path = neg_face_path + neu_face_path + pos_face_path\n",
    "        self.neg_face_path = neg_face_path\n",
    "        self.neu_face_path = neu_face_path\n",
    "        self.pos_face_path = pos_face_path\n",
    "        \n",
    "        self.maxFaces = maxFaces\n",
    "        self.Extract_Index = np.array(np.zeros(len(self.all_face_path)),dtype = np.int64)\n",
    "        self.Individual_Label = [[0 for i in range(self.maxFaces+2)] for i in range(len(self.label))]\n",
    "        for i in range(len(self.label)):\n",
    "            self.Individual_Label[i] = self.label[i] + self.Individual_Label[i]\n",
    "                \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.file_paths)) \n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.file_paths[idx]\n",
    "        g_f = np.load(img_path)\n",
    "        global_feature = g_f['global_feature']\n",
    "        individual_label = self.Individual_Label[idx]\n",
    "        maxFaces = self.maxFaces\n",
    "        #CROPPED FACE IMAGES\n",
    "        individual_faces = np.zeros((maxFaces + 2, 3, 224, 224), dtype = 'float32')\n",
    "        individual_label = self.Individual_Label[idx]\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(maxFaces):\n",
    "            face_path = self.all_face_path[idx] + '_' + str(i) + '.jpg'            \n",
    "            if os.path.exists(face_path) is False:\n",
    "                break\n",
    "            face = pil_loader(face_path)\n",
    "            counter = counter + 1\n",
    "                        \n",
    "            if self.transformFaces:\n",
    "                face = self.transformFaces(face)\n",
    "                \n",
    "            individual_faces[i] = face\n",
    "            \n",
    "        label = self.label[idx]\n",
    "        numberFaces = counter\n",
    "        \n",
    "        len_neg = len(self.neg_face_path)\n",
    "        len_neu = len(self.neu_face_path)\n",
    "        len_pos = len(self.pos_face_path)\n",
    "        len_all = len(self.all_face_path)\n",
    "        if numberFaces != 0:\n",
    "            if label == 0:\n",
    "                low_bound1 = len_neg\n",
    "                up_bound1= len_neu - 1 + len_neg\n",
    "                low_bound2 = len_neu + len_neg\n",
    "                up_bound2= len_all-1\n",
    "                individual_label[numberFaces] = 1\n",
    "                individual_label[numberFaces+1] = 2\n",
    "            elif label == 1:\n",
    "                low_bound1 = 0\n",
    "                up_bound1= len_neg-1\n",
    "                low_bound2 = len_neg + len_neu\n",
    "                up_bound2= len_all-1 \n",
    "                individual_label[numberFaces] = 0\n",
    "                individual_label[numberFaces+1] = 2\n",
    "            elif label == 2:\n",
    "                low_bound1 = 0\n",
    "                up_bound1= len_neg-1\n",
    "                low_bound2 = len_neg\n",
    "                up_bound2= len_neg + len_neu - 1                  \n",
    "                individual_label[numberFaces] = 0\n",
    "                individual_label[numberFaces+1] = 1\n",
    "            while(True):\n",
    "                temp_ind1 = random.randint(low_bound1,up_bound1)\n",
    "                face_path = self.all_face_path[temp_ind1] + '_' + str(self.Extract_Index[temp_ind1]) + '.jpg'\n",
    "                if os.path.exists(face_path):\n",
    "                    face = pil_loader(face_path)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "            if self.transformFaces:\n",
    "                face = self.transformFaces(face)\n",
    "                \n",
    "            individual_faces[numberFaces] = face\n",
    "  \n",
    "            while(True):\n",
    "                temp_ind2 = random.randint(low_bound2,up_bound2)\n",
    "                face_path = self.all_face_path[temp_ind2] + '_' + str(self.Extract_Index[temp_ind2]) + '.jpg'\n",
    "                if os.path.exists(face_path):\n",
    "                    face = pil_loader(face_path)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "            if self.transformFaces:\n",
    "                face = self.transformFaces(face)\n",
    "                \n",
    "            individual_faces[numberFaces+1] = face\n",
    "\n",
    "        #SAMPLE\n",
    "        return global_feature, individual_faces, individual_label, label, numberFaces, idx\n",
    "    \n",
    "def pil_loader(path):    # 一般采用pil_loader函数。\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EPOCH = 50   \n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GAF_2 Training')\n",
    "parser.add_argument('--outf', default='./model/', help='folder to output images and model checkpoints') \n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "classes = ('Negative', 'Neutral', 'Positive')\n",
    "\n",
    "\n",
    "train_faces_data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5115, 0.3799, 0.3297], std=[0.1825, 0.1584, 0.1492]),\n",
    "\n",
    "    ])\n",
    "\n",
    "val_faces_data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5115, 0.3799, 0.3297], std=[0.1825, 0.1584, 0.1492]),\n",
    "\n",
    "    ])\n",
    "\n",
    "test_faces_data_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5115, 0.3799, 0.3297], std=[0.1825, 0.1584, 0.1492]),\n",
    "    ])\n",
    "\n",
    "train_dataset = EmotiWDataset(image_filelist='../GAF_2_global_features/GAF_2_Train_800/', face_filelist='../CroppedFaces_image_low_quality/GAF_2_Train/',maxFaces = 16,transformFaces=train_faces_data_transform)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, shuffle=True, batch_size=4, num_workers=2, pin_memory=True)\n",
    "\n",
    "val_dataset = EmotiWDataset(image_filelist='../GAF_2_global_features/GAF_2_Val_800/', face_filelist='../CroppedFaces_image_low_quality/GAF_2_Val/',maxFaces = 16, transformFaces=val_faces_data_transform)\n",
    "\n",
    "validationloader = DataLoader(val_dataset, shuffle =False, batch_size = 256, num_workers = 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, model_1):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_heads = 1\n",
    "        self.features = model_1.features\n",
    "        self.fc1_layer = nn.Sequential(model_1.fc.fc6, nn.ReLU(), nn.Dropout(0.5))\n",
    "        self.fc2_layer = nn.Sequential(model_1.fc.fc7, nn.ReLU(), nn.Dropout(0.5))          \n",
    "        self.output_layer = nn.Sequential(model_1.fc.fc8)                \n",
    "        \n",
    "        # Self_fusion\n",
    "        self.Self_Fusion_layer = Self_Fusion_layer(imgf_dim = 1024, facef_dim = 4096)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        for k, layer in self.features.items():\n",
    "            x1 = layer(x1)\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        weights = torch.zeros(self.n_heads,x1.shape[0]).requires_grad_(requires_grad=True).cuda()\n",
    "        x1 = self.fc1_layer(x1)\n",
    "        x1 = self.fc2_layer(x1)\n",
    "        attention_weights = self.Self_Fusion_layer(x1,x2)\n",
    "        face_out = self.output_layer(x1) \n",
    "        return attention_weights, face_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Fusion_layer(nn.Module):\n",
    "    def __init__(self, imgf_dim, facef_dim):\n",
    "        super(Self_Fusion_layer, self).__init__()\n",
    "        # Self_Attention\n",
    "        self.face_fc_layer = nn.Sequential(nn.Linear(in_features=facef_dim, out_features=128), nn.ReLU())\n",
    "        self.img_fc_layer = nn.Sequential(nn.Linear(in_features=imgf_dim, out_features=128), nn.ReLU())\n",
    "        \n",
    "    def forward(self,x1,x2):\n",
    "        x1 = self.face_fc_layer(x1)\n",
    "        x2 = self.img_fc_layer(x2)\n",
    "        x2 = x2.repeat(x1.shape[0],1)\n",
    "        attention_weights = torch.cosine_similarity(x1,x2)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "vggface = VGGFace()\n",
    "vggface.load_state_dict(torch.load('pretrained/vggface.pth'))\n",
    "\n",
    "fc8_features = vggface.fc.fc8.in_features\n",
    "vggface.fc.fc8 = nn.Linear(fc8_features, 3)\n",
    "net = Net(vggface)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, FacesNet!\n",
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:1] Loss: 2.962 | Acc: 100.000% \n",
      "[epoch:1, iter:2] Loss: 2.991 | Acc: 50.000% \n",
      "[epoch:1, iter:3] Loss: 2.991 | Acc: 41.667% \n",
      "[epoch:1, iter:4] Loss: 3.010 | Acc: 37.500% \n",
      "[epoch:1, iter:5] Loss: 3.009 | Acc: 40.000% \n",
      "[epoch:1, iter:6] Loss: 3.006 | Acc: 37.500% \n",
      "[epoch:1, iter:7] Loss: 3.003 | Acc: 39.286% \n",
      "[epoch:1, iter:8] Loss: 3.002 | Acc: 40.625% \n",
      "[epoch:1, iter:9] Loss: 2.999 | Acc: 38.889% \n",
      "[epoch:1, iter:10] Loss: 3.006 | Acc: 35.000% \n",
      "[epoch:1, iter:11] Loss: 3.001 | Acc: 36.364% \n",
      "[epoch:1, iter:12] Loss: 2.994 | Acc: 37.500% \n",
      "[epoch:1, iter:13] Loss: 2.990 | Acc: 40.385% \n",
      "[epoch:1, iter:14] Loss: 2.992 | Acc: 41.071% \n",
      "[epoch:1, iter:15] Loss: 2.989 | Acc: 43.333% \n",
      "[epoch:1, iter:16] Loss: 2.988 | Acc: 43.750% \n",
      "[epoch:1, iter:17] Loss: 2.988 | Acc: 45.588% \n",
      "[epoch:1, iter:18] Loss: 2.987 | Acc: 45.833% \n",
      "[epoch:1, iter:19] Loss: 2.983 | Acc: 44.737% \n",
      "[epoch:1, iter:20] Loss: 2.984 | Acc: 45.000% \n",
      "[epoch:1, iter:21] Loss: 2.985 | Acc: 44.048% \n",
      "[epoch:1, iter:22] Loss: 2.986 | Acc: 43.182% \n",
      "[epoch:1, iter:23] Loss: 2.987 | Acc: 41.304% \n",
      "[epoch:1, iter:24] Loss: 2.985 | Acc: 42.708% \n",
      "[epoch:1, iter:25] Loss: 2.985 | Acc: 41.000% \n",
      "[epoch:1, iter:26] Loss: 2.982 | Acc: 42.308% \n",
      "[epoch:1, iter:27] Loss: 2.983 | Acc: 41.121% \n",
      "[epoch:1, iter:28] Loss: 2.981 | Acc: 42.342% \n",
      "[epoch:1, iter:29] Loss: 2.981 | Acc: 41.739% \n",
      "[epoch:1, iter:30] Loss: 2.982 | Acc: 41.176% \n",
      "[epoch:1, iter:31] Loss: 2.982 | Acc: 40.650% \n",
      "[epoch:1, iter:32] Loss: 2.982 | Acc: 39.370% \n",
      "[epoch:1, iter:33] Loss: 2.983 | Acc: 38.931% \n",
      "[epoch:1, iter:34] Loss: 2.982 | Acc: 38.519% \n",
      "[epoch:1, iter:35] Loss: 2.981 | Acc: 38.849% \n",
      "[epoch:1, iter:36] Loss: 2.977 | Acc: 39.860% \n",
      "[epoch:1, iter:37] Loss: 2.976 | Acc: 40.816% \n",
      "[epoch:1, iter:38] Loss: 2.977 | Acc: 39.735% \n",
      "[epoch:1, iter:39] Loss: 2.975 | Acc: 40.000% \n",
      "[epoch:1, iter:40] Loss: 2.976 | Acc: 39.623% \n",
      "[epoch:1, iter:41] Loss: 2.976 | Acc: 38.650% \n",
      "[epoch:1, iter:42] Loss: 2.976 | Acc: 38.323% \n",
      "[epoch:1, iter:43] Loss: 2.979 | Acc: 37.427% \n",
      "[epoch:1, iter:44] Loss: 2.977 | Acc: 38.286% \n",
      "[epoch:1, iter:45] Loss: 2.977 | Acc: 37.989% \n",
      "[epoch:1, iter:46] Loss: 2.978 | Acc: 37.705% \n",
      "[epoch:1, iter:47] Loss: 2.973 | Acc: 38.503% \n",
      "[epoch:1, iter:48] Loss: 2.973 | Acc: 38.220% \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8964ace7f440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m                             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                         \u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual_faces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumberFaces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                         \u001b[0mface_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ce721e69ad70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "base_params = list(map(id, net.features.parameters()))\n",
    "logits_params = filter(lambda p: id(p) not in base_params, net.parameters())\n",
    "\n",
    "\n",
    "params = [{'params': logits_params, 'lr':1e-5},\n",
    "            {'params': net.features.parameters(), 'lr':1e-5}]\n",
    "\n",
    "optimizer = optim.Adam(params,weight_decay = 1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "margin_1 = 0.8\n",
    "margin_2 = 0.6\n",
    "relabel_epoch = 10\n",
    "# Training\n",
    "if __name__ == \"__main__\":\n",
    "    best_acc = 77 \n",
    "    print(\"Start Training, FacesNet!\")\n",
    "    with open(\"FacesNet_GAF_2_acc.txt\", \"w\") as f:\n",
    "        with open(\"FacesNet_GAF_2_log.txt\", \"w\")as f2:\n",
    "            for epoch in range(EPOCH):\n",
    "                print('\\nEpoch: %d' % (epoch + 1))\n",
    "                net.train()\n",
    "                sum_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                sum_group_loss = 0.0\n",
    "                sum_rank_loss = 0.0\n",
    "                sum_loss_CL = 0.0\n",
    "                sum_individual_loss = 0.0\n",
    "                \n",
    "                for i, data in enumerate(trainloader, 0):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # 准备数据\n",
    "                    length = len(trainloader)\n",
    "                    \n",
    "                    global_feature, individual_faces, individual_label, labels, numberFaces, index = data\n",
    "                    \n",
    "                    ind = np.where(numberFaces==0)\n",
    "                    clear_labels = np.delete(labels, ind)\n",
    "                    \n",
    "                    global_feature, individual_faces, labels, clear_labels = global_feature.to(device),individual_faces.to(device), labels.to(device), clear_labels.to(device)\n",
    "                    individual_label = individual_label.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward + backward\n",
    "                    group_outputs = torch.zeros(np.count_nonzero(numberFaces),3).requires_grad_(requires_grad=True).to(device)\n",
    "                    loss_GCE = torch.zeros(np.count_nonzero(numberFaces)).requires_grad_(requires_grad=True).to(device)\n",
    "                    loss_CL = torch.zeros(np.count_nonzero(numberFaces)).requires_grad_(requires_grad=True).to(device)\n",
    "                    loss_ICE = torch.zeros(np.count_nonzero(numberFaces)).requires_grad_(requires_grad=True).to(device)\n",
    "                    count = 0\n",
    "                    \n",
    "                    if sum(numberFaces) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    for j in range(labels.shape[0]):          \n",
    "                        if numberFaces[j] == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        attention_weights, outputs = net(individual_faces[j,0:numberFaces[j]+2],global_feature[j])  \n",
    "                        face_norm = outputs\n",
    "\n",
    "                        loss_ICE[count] = criterion(outputs[:-2],individual_label[j][0:numberFaces[j]])\n",
    "                        \n",
    "                        # Contrastive Learning \n",
    "                        if numberFaces[j] > 1:\n",
    "                            temp_range = range(numberFaces[j]+2)\n",
    "                            temp_range1 = range(numberFaces[j])\n",
    "                            top_idx = torch.where(individual_label[j][temp_range]==labels[j])\n",
    "                            down_idx = torch.where(individual_label[j][temp_range]!=labels[j])\n",
    "                            high_group = attention_weights[top_idx]\n",
    "                            low_group = attention_weights[down_idx]\n",
    "                            high_mean = torch.mean(high_group)\n",
    "                            low_mean = torch.mean(low_group)\n",
    "                            diff  = low_mean - high_mean + margin_1\n",
    "                          \n",
    "                            weights_sum = attention_weights[:-2].sum(dim=0) + 1e-9\n",
    "                            if weights_sum < 1:\n",
    "                                weights_sum = 1\n",
    "                        \n",
    "                            weights_norm = 1/weights_sum * attention_weights[:-2]\n",
    "                            \n",
    "                            group_outputs[count] = torch.matmul(weights_norm[temp_range1].T, face_norm[temp_range1])\n",
    "                            outputs_norm = F.softmax(outputs[temp_range1],dim=1)\n",
    "                            _ ,top1_idx = torch.topk(outputs_norm[:,labels[j]].squeeze(),1, largest = True)\n",
    "                            trainloader.dataset.Extract_Index[index[j]] = top1_idx\n",
    "                            _, predicted = torch.max(outputs[:-2].data, 1)\n",
    "                                \n",
    "                            # Relabel samples\n",
    "                            if epoch + 1 >= relabel_epoch:\n",
    "                                sm = torch.softmax(outputs[temp_range1], dim = 1)\n",
    "                                Pmax, predicted_labels = torch.max(sm, 1) # predictions\n",
    "                                Pgt = torch.gather(sm, 1, individual_label[j][temp_range1].view(-1,1)).squeeze() # retrieve predicted probabilities of targets\n",
    "                                true_or_false = (Pmax - Pgt > margin_2)&(labels[j] != 1).to(device)\n",
    "                                update_idx = true_or_false.nonzero().squeeze() # get samples' index in this mini-batch where (Pmax - Pgt > margin_2)\n",
    "\n",
    "                                if update_idx.numel():\n",
    "                                    relabels = predicted_labels[update_idx] # predictions where (Pmax - Pgt > margin_2)\n",
    "                                    trainloader.dataset.Individual_Label[index[j]][update_idx.cpu()] = relabels.cpu()\n",
    "\n",
    "                        else:\n",
    "                            group_outputs[count] = face_norm[:-2]\n",
    "                            low_group = attention_weights[-2:-1]\n",
    "                            high_mean = attention_weights[0]\n",
    "                            low_mean = torch.mean(low_group)\n",
    "                            diff  = low_mean - high_mean + margin_1\n",
    "\n",
    "                        if diff > 0:\n",
    "                             loss_CL[count] = diff\n",
    "                        else:\n",
    "                             loss_CL[count] = 0.0\n",
    "                        \n",
    "                        count += 1\n",
    "                        \n",
    "                    loss_GCE = criterion(group_outputs, clear_labels)\n",
    "                    loss = loss_GCE + loss_CL.mean(dim=0) + loss_ICE.mean(dim=0)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # 每训练1个batch打印一次loss和准确率\n",
    "                    sum_loss += loss.item()\n",
    "                    sum_group_loss += loss_GCE.item()\n",
    "                    sum_loss_CL += loss_CL.mean(dim=0).item()\n",
    "                    sum_individual_loss += loss_ICE.mean(dim=0).item()\n",
    "                    _, predicted = torch.max(group_outputs.data, 1)\n",
    "                    total += clear_labels.size(0)\n",
    "                    correct += predicted.eq(clear_labels.data).cpu().sum()\n",
    "                    print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "                          % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "                    f2.write('%03d  %05d |Loss: %.03f | Acc: %.3f%% | Group Loss: %.3f | Individual Loss: %.3f | Contrastive Loss: %.3f '\n",
    "                          % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total, sum_group_loss/ (i + 1), sum_individual_loss/ (i + 1), sum_loss_CL/ (i + 1)))\n",
    "                    f2.write('\\n')\n",
    "                    f2.flush()\n",
    "  \n",
    "                scheduler.step()\n",
    "                torch.cuda.empty_cache()\n",
    "                # 每训练完一个epoch测试一下准确率\n",
    "                print(\"Waiting Test!\")\n",
    "                with torch.no_grad():\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    class_correct = list(0. for j in range(3)) # 定义一个存储每类中测试正确的个数的 列表，初始化为0\n",
    "                    class_total = list(0. for j in range(3))   # 定义一个存储每类中测试总数的个数的 列表，初始化为0\n",
    "                    for data in validationloader:\n",
    "                        net.eval()\n",
    "                        \n",
    "                        global_feature, individual_faces, individual_label, labels, numberFaces, index = data\n",
    "                        \n",
    "                        ind = np.where(numberFaces==0)\n",
    "                        clear_labels = np.delete(labels, ind).int()\n",
    "                        \n",
    "                        global_feature, individual_faces, labels, clear_labels = global_feature.to(device),individual_faces.to(device), labels.to(device), clear_labels.to(device)\n",
    "\n",
    "                        \n",
    "                        # forward + backward\n",
    "                        group_outputs = torch.zeros(np.count_nonzero(numberFaces),3).requires_grad_(requires_grad=True).to(device)\n",
    "                        \n",
    "                        count = 0\n",
    "                        for j in range(labels.shape[0]):          \n",
    "                            if numberFaces[j] == 0:\n",
    "                                continue\n",
    "                            label = labels[j]*torch.ones(numberFaces[j]).to(device)\n",
    "                            attention_weights, outputs = net(individual_faces[j,0:numberFaces[j]],global_feature[j])\n",
    "                            face_norm = outputs\n",
    "                            weights_sum = attention_weights.sum(dim=0) + 1e-9# 防止分母为零\n",
    "                            weights_norm = 1/weights_sum * attention_weights\n",
    "                            group_outputs[count] = torch.matmul(weights_norm.T, face_norm)\n",
    "                            count += 1                        \n",
    "                        \n",
    "                        torch.cuda.empty_cache()\n",
    "                        _, predicted = torch.max(group_outputs.data, 1)\n",
    "                        c = (predicted == clear_labels).squeeze() \n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == clear_labels).sum()                       \n",
    "                        if  clear_labels.shape[0] == 1:\n",
    "                            class_correct[clear_labels] += c\n",
    "                        else:\n",
    "                            for j in range(clear_labels.shape[0]):      # 因为每个batch都有多张图片，所以还需要一个小循环\n",
    "                                label = clear_labels[j]   # 对各个类的进行各自累加\n",
    "                                class_correct[label] += c[j]\n",
    "                                class_total[label] += 1    \n",
    "                        \n",
    "                    for j in range(3):\n",
    "                        print('Accuracy of %5s : %2d %%' % (\n",
    "                                classes[j], 100 * class_correct[j] // class_total[j]))\n",
    "                    print('Accuracy on val set：%.3f%%' % (100 * correct // total))\n",
    "                    acc = 100. * correct / total\n",
    "                    f.write(\"EPOCH=%03d,Accuracy= %.3f%%\" % (epoch + 1, acc))\n",
    "                    f.write('\\n')\n",
    "                    f.flush()\n",
    "                    if acc > best_acc:\n",
    "                        print('Saving model......')\n",
    "                        #torch.save(net.state_dict(), '%snet_%03d.pth' % (args.outf, epoch + 1))\n",
    "                        torch.save(net.state_dict(), 'GAF_2_best_net_face.pth')\n",
    "                        f3 = open(\"FacesNet_GAF_2_Self_Fusion_best_acc.txt\", \"w\")\n",
    "                        f3.write(\"EPOCH=%d,best_acc= %.3f%%\" % (epoch + 1, acc))\n",
    "                        f3.close()\n",
    "                        best_acc = acc\n",
    "\n",
    "            print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
